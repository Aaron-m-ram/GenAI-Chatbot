{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaMPBq5UKyb3"
      },
      "source": [
        "# Final Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CYTH4h2HKyb4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from textblob import TextBlob"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HJki_G2K22a",
        "outputId": "47ad9dfb-734c-41c7-835a-33ef1abade42"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bx2_s2AxKyb4"
      },
      "outputs": [],
      "source": [
        "# Show all columns\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# Show all rows (adjust if needed, can use 'None' for no limit)\n",
        "pd.set_option('display.max_rows', None)\n",
        "\n",
        "# Set the maximum width of each column (adjust if needed)\n",
        "pd.set_option('display.max_colwidth', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_3_YrA_AKyb5"
      },
      "outputs": [],
      "source": [
        "def load_movie_lines(lines_file_path):\n",
        "    lines = {}\n",
        "    with open(lines_file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split(' +++$+++ ')\n",
        "            if len(parts) == 5:\n",
        "                line_id, character_id, movie_id, character_name, text = parts\n",
        "                lines[line_id] = {\n",
        "                    \"character_id\": character_id,\n",
        "                    \"movie_id\": movie_id,\n",
        "                    \"character_name\": character_name,\n",
        "                    \"text\": text\n",
        "                }\n",
        "    return lines\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XR7_AlOuKyb5"
      },
      "outputs": [],
      "source": [
        "def load_movie_conversations(conversations_file_path):\n",
        "    conversations = []\n",
        "    with open(conversations_file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split(' +++$+++ ')\n",
        "            if len(parts) == 4:\n",
        "                character1_id, character2_id, movie_id, utterance_ids = parts\n",
        "                utterance_ids = utterance_ids[1:-1].replace(\"'\", \"\").split(', ')\n",
        "                conversations.append({\n",
        "                    \"character1_id\": character1_id,\n",
        "                    \"character2_id\": character2_id,\n",
        "                    \"movie_id\": movie_id,\n",
        "                    \"utterances\": utterance_ids\n",
        "                })\n",
        "    return conversations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8iNHBtT3Kyb5"
      },
      "outputs": [],
      "source": [
        "def load_movie_metadata(metadata_file_path):\n",
        "    movie_metadata = {}\n",
        "    with open(metadata_file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split(' +++$+++ ')\n",
        "            if len(parts) > 5:\n",
        "                movie_id, title, genre = parts[0], parts[1], parts[5]\n",
        "                movie_metadata[movie_id] = genre\n",
        "    return movie_metadata"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentiment(text):\n",
        "    sentiment = TextBlob(text).sentiment\n",
        "    return sentiment.polarity  # Returns a value between -1 (negative) and 1 (positive)"
      ],
      "metadata": {
        "id": "FvRu1ET2JEdx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "l9oX_92SKyb5"
      },
      "outputs": [],
      "source": [
        "# lines_file = \"./Cornell_Movie_Dialogue_Corpus/movie_lines.txt\"\n",
        "# conversation_file = \"./Cornell_Movie_Dialogue_Corpus/movie_conversations.txt\"\n",
        "# metadata_file = \"./Cornell_Movie_Dialogue_Corpus/movie_titles_metadata.txt\"\n",
        "lines_file = \"/content/drive/MyDrive/Colab Notebooks/AAI-520/Final Project/GenAI-Chatbot/Cornell_Movie_Dialogue_Corpus/movie_lines.txt\"\n",
        "conversation_file = \"/content/drive/MyDrive/Colab Notebooks/AAI-520/Final Project/GenAI-Chatbot/Cornell_Movie_Dialogue_Corpus/movie_conversations.txt\"\n",
        "metadata_file = \"/content/drive/MyDrive/Colab Notebooks/AAI-520/Final Project/GenAI-Chatbot/Cornell_Movie_Dialogue_Corpus/movie_titles_metadata.txt\"\n",
        "\n",
        "lines = load_movie_lines(lines_file)\n",
        "conversations = load_movie_conversations(conversation_file)\n",
        "movie_metadata = load_movie_metadata(metadata_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIYyuFVnKyb5"
      },
      "outputs": [],
      "source": [
        "# Build character ID to name mapping\n",
        "character_id_to_name = {}\n",
        "\n",
        "for line in lines.values():\n",
        "    character_id = line['character_id']\n",
        "    character_name = line['character_name']\n",
        "    character_id_to_name[character_id] = character_name\n",
        "\n",
        "# Initialize empty list to store conversation data\n",
        "data = []\n",
        "\n",
        "# Set the maximum history length\n",
        "MAX_HISTORY_LENGTH = 5\n",
        "\n",
        "# Define a special token for empty history\n",
        "EMPTY_HISTORY_TOKEN = \"[START]\"\n",
        "\n",
        "# Loop through each conversation block\n",
        "for conv in conversations:\n",
        "    utterance_ids = conv['utterances']\n",
        "    character1_id = conv['character1_id']\n",
        "    character2_id = conv['character2_id']\n",
        "    character1_name = character_id_to_name.get(character1_id)\n",
        "    character2_name = character_id_to_name.get(character2_id)\n",
        "\n",
        "    # Skip conversation if character names are missing\n",
        "    if not character1_name or not character2_name:\n",
        "        continue\n",
        "\n",
        "    # Initialize full conversation history\n",
        "    full_conversation_history = []\n",
        "\n",
        "    # Loop through the utterances to build conversation history and response for each block\n",
        "    for i in range(len(utterance_ids)):\n",
        "        response_id = utterance_ids[i]  # Current response ID\n",
        "\n",
        "        # Ensure the response_id is in the lines dictionary\n",
        "        if response_id in lines:\n",
        "            response_line = lines[response_id]\n",
        "            response = response_line['text']\n",
        "            character_2 = response_line['character_name']  # Current speaker\n",
        "\n",
        "            # Determine character_1 as the other character in the conversation\n",
        "            if character_2 == character1_name:\n",
        "                character_1 = character2_name\n",
        "            else:\n",
        "                character_1 = character1_name\n",
        "\n",
        "            # Build the conversation history using a sliding window\n",
        "            start_idx = max(0, i - MAX_HISTORY_LENGTH)\n",
        "            history_ids = utterance_ids[start_idx:i]\n",
        "\n",
        "            # Check if history is empty\n",
        "            if not history_ids:\n",
        "                conversation_history = [EMPTY_HISTORY_TOKEN]\n",
        "            else:\n",
        "                conversation_history = [lines[utt_id]['text'] for utt_id in history_ids if utt_id in lines]\n",
        "\n",
        "            # Determine the history length, setting it to 0 if [START] is in conversation_history\n",
        "            if conversation_history == [EMPTY_HISTORY_TOKEN]:\n",
        "                history_length = 0\n",
        "            else:\n",
        "                history_length = len(conversation_history)\n",
        "\n",
        "            # Append the conversation and its context to the data\n",
        "            data.append({\n",
        "                \"movie_id\": conv['movie_id'],\n",
        "                \"character_1\": character_1,  # Person being addressed\n",
        "                \"character_2\": character_2,  # Current speaker\n",
        "                \"conversation_history\": conversation_history,\n",
        "                \"history_length\": history_length,\n",
        "                \"response\": response,\n",
        "                \"genre\": movie_metadata.get(conv['movie_id'], [])\n",
        "            })\n",
        "\n",
        "            # Update full conversation history\n",
        "            full_conversation_history.append(response)\n",
        "        else:\n",
        "            continue  # Skip if the response_id is missing in lines\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNSQBJoSKyb6"
      },
      "outputs": [],
      "source": [
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Add Genre as meta data\n",
        "df['genre'] = df['movie_id'].map(movie_metadata)\n",
        "\n",
        "# Add sentiment to response\n",
        "df['sentiment'] = df['response'].apply(get_sentiment)\n",
        "\n",
        "# Display the DataFrame\n",
        "#df.head(130:140)\n",
        "df.iloc[128:141]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iW2vNFLMKyb6"
      },
      "outputs": [],
      "source": [
        "df.to_csv(\"/content/drive/MyDrive/Colab Notebooks/AAI-520/Final Project/GenAI-Chatbot/cleaned_conversations_no_genre.csv\", index=False)\n",
        "\n",
        "#\"/content/drive/MyDrive/Colab Notebooks/AAI-520/Final Project/GenAI-Chatbot/cleaned_conversations_ohe.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zaYAy8OKyb6"
      },
      "source": [
        "## EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Zsh1H3FKyb7"
      },
      "outputs": [],
      "source": [
        "#import matplotlib as plt\n",
        "import matplotlib.pyplot as plt\n",
        "import dask.dataframe as dd\n",
        "from dask.diagnostics import ProgressBar\n",
        "import seaborn as sns\n",
        "import ast\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Set seaborn style\n",
        "sns.set_theme(style='whitegrid')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHVbWXZiKyb7"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5KBZddmKyb7"
      },
      "outputs": [],
      "source": [
        "# Convert Pandas DataFrame to Dask DataFrame\n",
        "# Dask was needed due to the complexity with the conversations and its ability to parallel\n",
        "ddf = dd.from_pandas(df, npartitions=10)  # Adjust npartitions based on system capacity\n",
        "\n",
        "# Enable progress bar to monitor the process\n",
        "with ProgressBar():\n",
        "    # Run describe on all columns, including text-heavy ones\n",
        "    summary = ddf.describe(include='all').compute()\n",
        "\n",
        "# Display the summary statistics\n",
        "print(summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DvE5vrRKyb7"
      },
      "source": [
        "The dataset highlights brief interaction patterns and a diversity of responses, which are crucial for training models to understand and generate contextually appropriate dialogue. With a wide variety of genres and character dynamics represented, particularly from dramatic contexts, the model can be better equipped to manage nuanced and emotionally varied conversations, reflecting real-world scenarios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulgwVcA8Kyb7"
      },
      "outputs": [],
      "source": [
        "# Step 1: Correct history length for true zero-length histories (conversation_history == [] and response is not empty)\n",
        "df['history_length'] = df.apply(lambda row: 0 if row['conversation_history'] == [] and row['response'].strip() != '' else row['history_length'], axis=1)\n",
        "\n",
        "# Step 2: Exclude entries where conversation history is [\"START\"]\n",
        "filtered_df = df[df['conversation_history'].apply(lambda x: x != [\"START\"])]\n",
        "\n",
        "# Step 3: Plot the distribution of conversation history lengths\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns_plot = sns.countplot(x='history_length', data=filtered_df)\n",
        "\n",
        "# Add counts on top of each bar\n",
        "for p in sns_plot.patches:\n",
        "    sns_plot.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                      ha='center', va='baseline', fontsize=10, color='black', xytext=(0, 5),\n",
        "                      textcoords='offset points')\n",
        "\n",
        "# Add labels and title\n",
        "plt.title('Distribution of Conversation History Length (Excluding [\"START\"])')\n",
        "plt.xlabel('History Length')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PHyALlkKyb7"
      },
      "source": [
        "- 0 = 45\n",
        "- 1 = 38011\n",
        "- 2 = 17430\n",
        "- 3 = 9400\n",
        "\n",
        "4 and 5 are unkonwn due to the rolling window which spikes up 5 but 4 would be lower than 18150"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkeRB1jjKyb7"
      },
      "source": [
        "\n",
        "The high counts at 0 and 1 history length suggest that many conversations are either starting fresh or rely on minimal context, indicating that the chatbot needs to be adept at handling short interactions. The high 0 is due to all conversation starting at 0. The 45 discrepancy between 0 and 1 show that there are very minimal conversations that have absolutely no context. The spike at 5 history length likely reflects a rolling window effect, where conversations are capped at the most recent five turns. This ensures the chatbot focuses on the latest dialogue, optimizing response generation without handling overly long histories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yOtVLKoKyb8"
      },
      "outputs": [],
      "source": [
        "# Combine character_1 and character_2 to find most frequent characters\n",
        "character_counts = pd.concat([df['character_1'], df['character_2']]).value_counts().head(20)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=character_counts.values, y=character_counts.index)\n",
        "plt.title('Top 20 Most Frequent Characters')\n",
        "plt.xlabel('Number of Lines')\n",
        "plt.ylabel('Character Name')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1I_DAjtKyb8"
      },
      "source": [
        "Jack leads significantly with over 6,000 lines, followed by Joe and George, who both have under 4,000 lines. The distribution suggests that a few characters dominate the dataset, which may indicate that these characters are central to many conversations. The model will likely need to handle frequent interactions involving these high-volume characters, ensuring it can respond consistently and contextually to their dialogues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rII_DqVJKyb8"
      },
      "outputs": [],
      "source": [
        "# Convert the genre strings to actual lists\n",
        "df['genre'] = df['genre'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
        "\n",
        "# Step 1: Explode the 'genre' column so each genre gets its own row\n",
        "df_exploded = df.explode('genre')\n",
        "\n",
        "# Step 2: Count the occurrences of each genre\n",
        "#genre_counts = df_exploded['genre'].value_counts().head(20)  # Limit to top 20 genres\n",
        "genre_counts = df_exploded['genre'].value_counts() # Limit to top 20 genres\n",
        "\n",
        "# Step 3: Plot the genre distribution as a bar plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=genre_counts.values, y=genre_counts.index, palette=\"coolwarm\")\n",
        "plt.title('Top 20 Genres by Number of Conversations')\n",
        "plt.xlabel('Number of Conversations')\n",
        "plt.ylabel('Genre')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H28yX9-zKyb8"
      },
      "source": [
        "Drama, thriller, and comedy dominate the list, with drama leading by a significant margin, followed by thriller. These three genres make up the majority of conversations, indicating that the dataset skews toward emotionally intense and suspenseful dialogue. The model will need to adapt to different tones and conversation styles, especially handling serious and suspenseful conversations more frequently than others. Other genres like romance and crime also have a notable presence, adding further variety to the interactions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjPOGqJxKyb8"
      },
      "outputs": [],
      "source": [
        "# Add a column for response length\n",
        "df['response_length'] = df['response'].apply(lambda x: len(x.split()))\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(x='history_length', y='response_length', data=df)\n",
        "plt.title('History Length vs. Response Length')\n",
        "plt.xlabel('History Length')\n",
        "plt.ylabel('Response Length (words)')\n",
        "plt.show()\n",
        "\n",
        "# need to make sure this is words and not characters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dY1EVqlxKyb8"
      },
      "source": [
        "This scatter plot shows the relationship between history length (number of previous turns) and response length (in words). Most responses, regardless of history length, tend to be concise, clustering below 100 words. However, there are outliers where responses exceed 300 or even 500 words, especially when the history length is 0 or 1. This suggests that while most responses are brief, some dialogues start with a long monologue or have more detailed responses, which may require the model to handle a wide range of response lengths effectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ip5FrjGQKyb8"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Combine all responses into one string\n",
        "all_responses = ' '.join(df['response'].tolist())\n",
        "\n",
        "# Tokenize and remove stop words\n",
        "words = [word.lower() for word in all_responses.split() if word.isalpha() and word.lower() not in stop_words]\n",
        "\n",
        "word_counts = Counter(words).most_common(20)\n",
        "\n",
        "# Convert to DataFrame for plotting\n",
        "words_df = pd.DataFrame(word_counts, columns=['word', 'count'])\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x='count', y='word', data=words_df)\n",
        "plt.title('Top 20 Most Common Words in Responses')\n",
        "plt.xlabel('Count')\n",
        "plt.ylabel('Word')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ma8qrqq1Kyb8"
      },
      "source": [
        "This bar chart displays the Top 20 most common words found in responses after removing stopwords. Words like \"like,\" \"get,\" and \"know\" are the most frequent, indicating that the conversations often involve casual, everyday dialogue. Many of the words, such as \"want,\" \"think,\" and \"going,\" suggest that a significant portion of the conversations are centered around decision-making and opinions. The frequency of these basic verbs and pronouns highlights the conversational nature of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Plotting the sentiment distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df['sentiment'], bins=20, kde=True)\n",
        "plt.title('Sentiment Distribution')\n",
        "plt.xlabel('Sentiment Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True)\n",
        "plt.axvline(x=0, color='red', linestyle='--')  # Add a vertical line at sentiment = 0 for reference\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "A6shYZ35QPXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsslwiMtKyb9"
      },
      "source": [
        "The data primarily deals with neutral dialogues. While strong emotions are present, they are less frequent, so the model should prioritize neutral sentiment understanding with occasional adjustments for emotional responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmanLT0fKyb9"
      },
      "outputs": [],
      "source": [
        "# Extract bigrams\n",
        "vectorizer = CountVectorizer(ngram_range=(2, 2), stop_words='english')\n",
        "X = vectorizer.fit_transform(df['response'])\n",
        "bigram_counts = {word: count for word, count in zip(vectorizer.get_feature_names_out(), X.sum(axis=0).A1)}\n",
        "\n",
        "# Get the top 20 bigrams\n",
        "top_bigrams = Counter(bigram_counts).most_common(20)\n",
        "\n",
        "# Convert to DataFrame for plotting\n",
        "bigram_df = pd.DataFrame(top_bigrams, columns=['bigram', 'count'])\n",
        "\n",
        "# Plot the top 20 bigrams\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x='count', y='bigram', data=bigram_df)\n",
        "plt.title('Top 20 Bigrams in Responses')\n",
        "plt.xlabel('Count')\n",
        "plt.ylabel('Bigram')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZawvY277Kyb9"
      },
      "source": [
        "The frequent appearance of these common phrases suggests that the dataset is filled with casual, informal speech, which is typical for movie dialogues. The model will need to handle such conversational patterns, particularly those involving negative expressions and common phrases about knowledge or intention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQVfTSG8Kyb9"
      },
      "outputs": [],
      "source": [
        "# Reset the index after exploding to avoid issues with duplicate indices\n",
        "df_exploded = df.explode('genre').reset_index(drop=True)\n",
        "\n",
        "plt.figure(figsize=(18, 9))\n",
        "sns.boxplot(x='genre', y='response_length', data=df_exploded)\n",
        "plt.title('Response Length by Genre')\n",
        "plt.xlabel('Genre')\n",
        "plt.ylabel('Response Length (words)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmsyscLjKyb9"
      },
      "source": [
        "The model will need to handle both short responses (which dominate the dataset) and occasional long responses that vary by genre. The presence of long outliers in certain genres like drama and history suggests that more emotionally or narratively complex genres may require more detailed and nuanced responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dA2FaZGQKyb9"
      },
      "outputs": [],
      "source": [
        "# Combine all responses into one string\n",
        "all_responses = ' '.join(df['response'].tolist())\n",
        "\n",
        "# Create a word cloud\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_responses)\n",
        "\n",
        "# Plot the word cloud\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Word Cloud of Responses')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gw_3kM54Kyb9"
      },
      "source": [
        "This word cloud highlights the most common words in responses, with terms like \"know,\" \"think,\" and \"want\" standing out. These words suggest that many dialogues involve expressions of thought, desire, or action. The chatbot will need to handle everyday conversational language, focusing on these common themes in dialogue."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsISI772Kyb-"
      },
      "outputs": [],
      "source": [
        "# Example of tagging one response\n",
        "df['pos_tags'] = df['response'].apply(lambda x: nltk.pos_tag(nltk.word_tokenize(x)))\n",
        "\n",
        "# Count the frequency of each POS tag\n",
        "pos_counts = Counter([tag for tags in df['pos_tags'] for _, tag in tags])\n",
        "\n",
        "# Convert to DataFrame for plotting\n",
        "pos_df = pd.DataFrame(pos_counts.items(), columns=['POS', 'count'])\n",
        "\n",
        "# Plot the POS tag distribution\n",
        "plt.figure(figsize=(18, 9))\n",
        "sns.barplot(x='count', y='POS', data=pos_df)\n",
        "plt.title('Part of Speech Tag Distribution')\n",
        "plt.xlabel('Count')\n",
        "plt.ylabel('POS Tag')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Drop the 'pos_tags' column after plotting\n",
        "df.drop(columns=['pos_tags'], inplace=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tvovHF_Kyb-"
      },
      "source": [
        "The **Part of Speech (POS) Tag Distribution** graph highlights the dominance of nouns (NN), verbs (VB), and pronouns (PRP) in the dataset, suggesting a strong focus on actions and entities in the dialogues. Proper nouns (NNP) also appear frequently, indicating a focus on specific names and entities. The diversity of POS tags, including adjectives, conjunctions, and determiners, reflects the varied linguistic structure in the conversations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENd8GFH-Kyb-"
      },
      "outputs": [],
      "source": [
        "# Select numerical columns (including any other numeric features like sentiment, response length, etc.)\n",
        "numerical_columns = ['history_length', 'response_length', 'sentiment']  # Add more columns as needed\n",
        "\n",
        "# Compute the correlation matrix\n",
        "corr_matrix = df[numerical_columns].corr()\n",
        "\n",
        "# Plot the heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", linewidths=0.5, vmin=-1, vmax=1)\n",
        "plt.title('Feature Correlation Heatmap')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEvRip7GKyb-"
      },
      "source": [
        "The Feature Correlation Heatmap shows weak correlations between the features in the dataset: history length, response length, and sentiment. The correlation between history length and response length is minimal at 0.047, indicating that longer conversation histories do not significantly influence response lengths. The correlation between history length and sentiment is nearly negligible at 0.0087, suggesting that sentiment is independent of conversation length. Overall, the heatmap indicates that these features are largely uncorrelated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eW8xbMCnKyb-"
      },
      "source": [
        "### Summary of EDA\n",
        "\n",
        "Our analysis of the Cornell Movie-Dialog Corpus revealed several important insights regarding the structure and content of the dataset. The **conversation history length** showed that most dialogues are short, with 1 turn exchanges dominating, but a significant portion also had a rolling window of up to 5 turns, indicating some complexity in multi-turn conversations. The **POS tag distribution** demonstrated that the dataset is rich in nouns, verbs, and proper nouns, reflecting a focus on specific entities and actions in the dialogues. Moreover, the **word frequency and bigram analyses** highlighted the conversational nature of the dataset, with common informal phrases like \"don’t know\" and \"got it\" dominating the responses.\n",
        "\n",
        "The **sentiment distribution** revealed that most responses are neutral, with a concentration around 0, indicating that dialogues are not highly polarized. Meanwhile, the **response lengths** varied across genres but generally tended to be short, with occasional longer responses scattered across different genres. The **feature correlation analysis** showed weak or negligible correlations between history length, response length, and sentiment, suggesting that these factors operate independently in the dataset.\n",
        "\n",
        "### Best GenAI Model for This Task:\n",
        "\n",
        "Given the nature of the dataset—dominated by short, informal conversations with a range of context lengths—a **Seq2Seq model** such as **T5 (Text-to-Text Transfer Transformer)** would be well-suited for this task. The model’s flexibility in handling multi-turn dialogues and generating coherent responses makes it a strong choice for this dataset. T5's ability to handle text generation, alongside its capacity to condition on conversation history, will allow the chatbot to maintain contextual relevance across multiple turns. Additionally, fine-tuning T5 on this dataset would help the model understand the conversational dynamics and nuances, such as sentiment shifts and genre-specific responses.\n",
        "\n",
        "In summary, the **T5 model** offers the right balance of flexibility, generative capacity, and contextual understanding, making it the ideal choice for developing a conversational AI chatbot based on the Cornell Movie-Dialog Corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZm3BrEIKyb-"
      },
      "source": [
        "## T5 Modeling"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from torch.amp import autocast, GradScaler\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.optim import AdamW\n",
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-small')"
      ],
      "metadata": {
        "id": "DqcglwAT85JI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(conversation_history, response, sentiment):\n",
        "    input_text = \" \".join(conversation_history) + f\" Sentiment: {sentiment}\"\n",
        "    target_text = response\n",
        "    return input_text, target_text"
      ],
      "metadata": {
        "id": "X1Cx8kIM88oo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['input'], df['target'] = zip(*df.apply(lambda row: preprocess_data(row['conversation_history'], row['response'], row['sentiment']), axis=1))\n",
        "\n",
        "# Split the data into train, validation, and test sets\n",
        "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)"
      ],
      "metadata": {
        "id": "XeXPHItF8-ug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConversationDataset(Dataset):\n",
        "  def __init__(self, tokenizer, df, max_length=512):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.data = df\n",
        "    self.max_length = max_length\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    input_text = self.data.iloc[idx]['input']\n",
        "    target_text = self.data.iloc[idx]['target']\n",
        "\n",
        "    # Tokenize input and target\n",
        "\n",
        "    input_encodings = self.tokenizer(input_text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors=\"pt\")\n",
        "    target_encodings = self.tokenizer(target_text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors=\"pt\")\n",
        "\n",
        "    return {\n",
        "        'input_ids': input_encodings['input_ids'].flatten(),\n",
        "        'attention_mask': input_encodings['attention_mask'].flatten(),\n",
        "        'labels': target_encodings['input_ids'].flatten()\n",
        "    }"
      ],
      "metadata": {
        "id": "56bEmRT08-sE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create datasets\n",
        "train_dataset = ConversationDataset(tokenizer, train_df)\n",
        "val_dataset = ConversationDataset(tokenizer, val_df)\n",
        "test_dataset = ConversationDataset(tokenizer, test_df)\n",
        "\n",
        "# Create dataloaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)"
      ],
      "metadata": {
        "id": "ByaTiUsM8-pc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Gradient accumulation settings\n",
        "accumulation_steps = 4  # Number of steps to accumulate before updating gradients\n",
        "\n",
        "# Initialize GradScaler for mixed precision training\n",
        "scaler = GradScaler(device)\n",
        "\n",
        "# Scheduler to reduce learning rate if validation loss plateaus\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
        "\n",
        "# Training loop with gradient accumulation\n",
        "num_epochs = 3\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    start_time = time.time()  # Record start time of the epoch\n",
        "\n",
        "    # Training loop with tqdm progress bar\n",
        "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\", unit=\"batch\")\n",
        "\n",
        "    for i, batch in enumerate(progress_bar):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Move tensors to the appropriate device\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        # Forward pass with autocast for mixed precision\n",
        "        with autocast('cuda'):\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss / accumulation_steps  # Divide loss by accumulation steps\n",
        "\n",
        "        # Backward pass with gradient scaling\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # Accumulate gradients and update weights after `accumulation_steps` mini-batches\n",
        "        if (i + 1) % accumulation_steps == 0:\n",
        "            scaler.step(optimizer)  # Unscales gradients and performs optimizer step\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        running_loss += loss.item() * accumulation_steps  # Accumulate the original loss\n",
        "\n",
        "        # Update progress bar with the current loss\n",
        "        progress_bar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "    # Calculate time taken for the epoch\n",
        "    epoch_time = time.time() - start_time\n",
        "    avg_loss = running_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs} completed. Average loss: {avg_loss:.4f}. Time taken: {epoch_time:.2f} seconds.\")\n",
        "\n",
        "    # Validation after each epoch\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            val_loss += outputs.loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_dataloader)\n",
        "    print(f\"Validation Loss after Epoch {epoch + 1}: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # Step the scheduler based on validation loss\n",
        "    scheduler.step(avg_val_loss)\n",
        "\n"
      ],
      "metadata": {
        "id": "oDEikv_18-nA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "things i've test to make it faster\n",
        "  * batch size has to be at 16 due to limitations of computation\n",
        "  * implememented gradient accumulation to help cut time\n",
        "  * Mixed precision training FP16\n",
        "  * Reduce LR on Plateau\n",
        "  * need to test torch profiler\n",
        "  * A100 GPU works best\n",
        "  * works with 48 Unites\n",
        "  * added sentiment\n",
        "  * added classification of multilabels\n"
      ],
      "metadata": {
        "id": "3lRVo9KTJ0V1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the model\n",
        "model_save_path = \"/content/drive/MyDrive/Colab Notebooks/AAI-520/Final Project/GenAI-Chatbot/model_w_sent_t-5base.pth\"\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "print(f\"Model saved to {model_save_path}\")"
      ],
      "metadata": {
        "id": "f1b5MoU08-ko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing phase\n",
        "model.eval()\n",
        "test_loss = 0.0\n",
        "predictions = []\n",
        "true_labels = []\n",
        "\n",
        "start_time = time.time()  # Record start time for the evaluation\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Use tqdm to monitor progress\n",
        "    progress_bar = tqdm(test_dataloader, desc=\"Evaluating\", unit=\"batch\")\n",
        "    for batch in progress_bar:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        test_loss += outputs.loss.item()\n",
        "\n",
        "        # Store the predicted and true labels for further analysis\n",
        "        logits = outputs.logits\n",
        "        predicted_ids = torch.argmax(logits, dim=-1)\n",
        "        predictions.extend(predicted_ids.cpu().numpy())\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Calculate average test loss\n",
        "avg_test_loss = test_loss / len(test_dataloader)\n",
        "print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
        "\n",
        "# # Convert predictions and true_labels back to text\n",
        "# predicted_texts = [tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions]\n",
        "# true_texts = [tokenizer.decode(true, skip_special_tokens=True) for true in true_labels]\n",
        "\n",
        "# Calculate and print evaluation time\n",
        "evaluation_time = time.time() - start_time\n",
        "print(f\"Evaluation completed in {evaluation_time:.2f} seconds.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "chFsOQ4xgWCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate perplexity\n",
        "perplexity = torch.exp(torch.tensor(avg_test_loss))\n",
        "print(f\"Perplexity: {perplexity.item():.4f}\")"
      ],
      "metadata": {
        "id": "WTxQl80s71g7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Needed for Bleu Rogue and more. Doing it separate to not mess with other test\n",
        "\n",
        "# Start timing for the decoding process\n",
        "start_time = time.time()\n",
        "\n",
        "# Decode predicted texts\n",
        "predicted_texts = []\n",
        "for pred in tqdm(predictions, desc=\"Decoding Predicted Texts\"):\n",
        "    predicted_text = tokenizer.decode(pred, skip_special_tokens=True)\n",
        "    predicted_texts.append(predicted_text)\n",
        "\n",
        "# Decode true texts\n",
        "true_texts = []\n",
        "for true in tqdm(true_labels, desc=\"Decoding True Texts\"):\n",
        "    true_text = tokenizer.decode(true, skip_special_tokens=True)  # No need for double brackets here\n",
        "    true_texts.append(true_text)\n",
        "\n",
        "# Calculate and print total decoding time\n",
        "total_decoding_time = time.time() - start_time\n",
        "print(f\"Decoding completed in {total_decoding_time:.2f} seconds.\")\n"
      ],
      "metadata": {
        "id": "XMltaUfI-QCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "# Start timing for the BLEU score calculation\n",
        "start_time = time.time()\n",
        "\n",
        "# Initialize a list to store BLEU scores for each prediction\n",
        "bleu_scores = []\n",
        "\n",
        "# Create an instance of the smoothing function\n",
        "smoothing_function = SmoothingFunction()\n",
        "\n",
        "# Calculate BLEU scores with smoothing using unigram (1-gram) only\n",
        "for pred_text, true_text in tqdm(zip(predicted_texts, true_texts), total=len(predicted_texts), desc=\"Calculating BLEU Score\"):\n",
        "    score = sentence_bleu([[true_text]], pred_text.split(), weights=(1, 0, 0, 0), smoothing_function=smoothing_function.method1)  # Unigram only\n",
        "    bleu_scores.append(score)\n",
        "\n",
        "# Calculate average BLEU score\n",
        "average_bleu_score = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0\n",
        "\n",
        "# Print results\n",
        "print(f\"Average BLEU Score (Unigram): {average_bleu_score:.4f}\")\n",
        "\n",
        "# Calculate and print total BLEU score calculation time\n",
        "total_bleu_time = time.time() - start_time\n",
        "print(f\"BLEU score calculation completed in {total_bleu_time:.2f} seconds.\")\n"
      ],
      "metadata": {
        "id": "VpYlMJtCGVRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    pred_tokens = set(predicted_texts[i].split())\n",
        "    true_tokens = set(true_texts[i].split())\n",
        "    overlap = pred_tokens.intersection(true_tokens)\n",
        "    print(f\"Predicted Tokens: {pred_tokens}\")\n",
        "    print(f\"True Tokens: {true_tokens}\")\n",
        "    print(f\"Overlap: {overlap}\")\n",
        "    print(\"---\")\n"
      ],
      "metadata": {
        "id": "R1mbqIGbGnVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install rouge"
      ],
      "metadata": {
        "id": "rJVOxqzqibcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge import Rouge\n",
        "\n",
        "# Initialize ROUGE scorer\n",
        "rouge = Rouge()\n",
        "\n",
        "# Start timing for the ROUGE score calculation\n",
        "start_time = time.time()\n",
        "\n",
        "# Initialize lists to store ROUGE scores\n",
        "rouge_scores = []\n",
        "\n",
        "# Calculate ROUGE scores\n",
        "for pred_text, true_text in tqdm(zip(predicted_texts, true_texts), total=len(predicted_texts), desc=\"Calculating ROUGE Score\"):\n",
        "    score = rouge.get_scores(pred_text, true_text, avg=True)  # Calculate ROUGE scores\n",
        "    rouge_scores.append(score)\n",
        "\n",
        "# Convert the list of scores into a summary\n",
        "average_rouge = {\n",
        "    'rouge-1': sum(score['rouge-1']['f'] for score in rouge_scores) / len(rouge_scores),\n",
        "    'rouge-2': sum(score['rouge-2']['f'] for score in rouge_scores) / len(rouge_scores),\n",
        "    'rouge-l': sum(score['rouge-l']['f'] for score in rouge_scores) / len(rouge_scores),\n",
        "}\n",
        "\n",
        "# Print results\n",
        "print(\"Average ROUGE Scores:\")\n",
        "print(f\"ROUGE-1: {average_rouge['rouge-1']:.4f}\")\n",
        "print(f\"ROUGE-2: {average_rouge['rouge-2']:.4f}\")\n",
        "print(f\"ROUGE-L: {average_rouge['rouge-l']:.4f}\")\n",
        "\n",
        "# Calculate and print total ROUGE score calculation time\n",
        "total_rouge_time = time.time() - start_time\n",
        "print(f\"ROUGE score calculation completed in {total_rouge_time:.2f} seconds.\")\n"
      ],
      "metadata": {
        "id": "8KLqKTofG69v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.meteor_score import meteor_score\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Start timing for the METEOR score calculation\n",
        "start_time = time.time()\n",
        "\n",
        "# Initialize a list to store METEOR scores\n",
        "meteor_scores = []\n",
        "\n",
        "# Calculate METEOR scores\n",
        "for pred_text, true_text in tqdm(zip(predicted_texts, true_texts), total=len(predicted_texts), desc=\"Calculating METEOR Score\"):\n",
        "    # Tokenize the texts\n",
        "    pred_tokens = pred_text.split()  # Tokenize predicted text\n",
        "    true_tokens = true_text.split()   # Tokenize true text\n",
        "\n",
        "    # Calculate METEOR score\n",
        "    score = meteor_score([true_tokens], pred_tokens)  # Wrap true_tokens in a list for references\n",
        "    meteor_scores.append(score)\n",
        "\n",
        "# Calculate average METEOR score\n",
        "average_meteor_score = sum(meteor_scores) / len(meteor_scores) if meteor_scores else 0\n",
        "\n",
        "# Print results\n",
        "print(f\"Average METEOR Score: {average_meteor_score:.4f}\")\n",
        "\n",
        "# Calculate and print total METEOR score calculation time\n",
        "total_meteor_time = time.time() - start_time\n",
        "print(f\"METEOR score calculation completed in {total_meteor_time:.2f} seconds.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "fHEoAyQtHaiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H6no0zaJelnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Set the model to evaluation mode and move it to the appropriate device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Define a function to generate responses\n",
        "def generate_response(input_text):\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)  # Move input_ids to the same device as the model\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation for inference\n",
        "        output = model.generate(input_ids, max_length=50, num_return_sequences=1)  # Adjust max_length as needed\n",
        "\n",
        "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "# Continuous interaction loop\n",
        "print(\"Chatbot is ready! Type 'exit' to end the conversation.\")\n",
        "while True:\n",
        "    input_text = input(\"You: \")  # Get user input\n",
        "    if input_text.lower() == 'exit':\n",
        "        print(\"Ending conversation.\")\n",
        "        break  # Exit the loop if the user types 'exit'\n",
        "\n",
        "    generated_response = generate_response(input_text)  # Generate a response\n",
        "    print(f\"Chatbot: {generated_response}\")  # Print the response\n"
      ],
      "metadata": {
        "id": "Ht7WsJA-JRAu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
      }
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}